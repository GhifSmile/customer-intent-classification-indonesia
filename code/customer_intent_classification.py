# -*- coding: utf-8 -*-
"""customer-intent-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ST2TrEQTCbzmrRd70Fp1BQOulIfe99YX
"""

! pip install transformers==4.28.0
! pip install accelerate
! pip install datasets
! pip install rouge-score nltk
! pip install huggingface_hub
! pip install sentencepiece
! pip install evaluate
! pip install gdown
! pip install git-lfs

#!pip install indoNLP
!pip install Sastrawi
#!pip install spacy
#!pip install spacy-lookups-data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string

import warnings
warnings.filterwarnings('ignore')

import nltk
nltk.download('punkt')

"""# Load Datasets"""

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset
dataset = load_dataset("bitext/customer-support-intent-dataset")

print(dataset)

train = dataset["train"].to_pandas()
#validation = dataset["validation"].to_pandas()
#test = dataset["test"].to_pandas()

train

validation

test

"""## Translate en to id"""

from textblob import TextBlob
from textblob.exceptions import NotTranslated

def trans(sentence):

  try:
    sentence = sentence.lower()
    blob = TextBlob(sentence.strip())
    translation = str(blob.translate(from_lang='en',to='id'))

    return translation

  except NotTranslated:
    return sentence

train['utterance'] = train['utterance'].apply(lambda x: trans(x))

validation['utterance'] = validation['utterance'].apply(lambda x: trans(x))

test['utterance'] = test['utterance'].apply(lambda x: trans(x))

train

validation

test

"""## Save new datasets"""

train.to_csv('/content/drive/MyDrive/DSC dataset/df_train.csv', index=False)

validation.to_csv('/content/drive/MyDrive/DSC dataset/df_validation.csv', index=False)

test.to_csv('/content/drive/MyDrive/DSC dataset/df_test.csv', index=False)

"""# EDA"""

train_df = pd.read_csv('/content/drive/MyDrive/DSC dataset/df_train_modeling.csv')

train_df

train_df['length'] = train_df.utterance.apply(lambda x: len(x.split()))

train_df.info()

train_df.describe(exclude='object').T

"""Maksimum dan minimum kata pada data masing-masing 11 kata dan 2 kata"""

train_df.utterance[train_df.length==11]

train_df.describe(exclude='number').T

train_df.duplicated().sum()

print("Jumlah kategori intent unique: %d \n" % train_df.intent.nunique())
print(train_df.intent.unique())

print("Jumlah kategori unique: %d \n" % train_df.category.nunique())
print(train_df.category.unique())

custom_order = train_df.category.value_counts().nlargest(5).index

strong_color = '#FF5733'

palette = [strong_color if cat == custom_order[0] else sns.color_palette("magma")[0] for cat in custom_order]

plt.figure(figsize=(15, 10))
plt.title('jumlah pada data setiap category')
sns.countplot(data=train_df, y='category', order=custom_order, palette=palette)
#plt.xticks(rotation = 45)
plt.plot()

"""Kebanyakan masalah di kategori Account"""

custom_order = train_df.intent[train_df['category']=='ACCOUNT'].value_counts().index

strong_color = '#FF5733'

palette = [strong_color if cat == custom_order[0] else sns.color_palette("magma")[0] for cat in custom_order]

plt.figure(figsize=(15, 10))
plt.title('jumlah setiap intent pada category account')
sns.countplot(data=train_df, y=train_df.intent[train_df['category']=='ACCOUNT'], order=custom_order, palette=palette)
#plt.xticks(rotation = 45)
plt.plot()

"""Lebih spesifik pada masalah create account"""

custom_order = train_df.intent.value_counts().index

sns.set_theme(style="whitegrid")

plt.figure(figsize=(15, 10))
plt.title('Maksimum dan Minimum jumlah data pada setiap intent')
sns.countplot(data=train_df[train_df['intent'].isin([custom_order[0], custom_order[-1]])], x='intent', hue='category', order=[custom_order[0], custom_order[-1]])
plt.xticks(rotation=45)
plt.plot()

sns.reset_orig()

for i,j in enumerate(train_df.intent.unique()):

  print('{}. INTENT: {} CATEGORY: {} \n'.format(i+1, j, train_df['category'][train_df.intent == j].unique()))

train_df.intent.value_counts()

"""## Wordcloud"""

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

wordCloud_data = train_df.copy()
wordCloud_data['utterance'] = wordCloud_data['utterance'].apply(lambda x: x.lower())

def word_cloud(data):

  text = ' '.join([text for text in data])

  # Create stopword list:
  stopwords = set(StopWordRemoverFactory().get_stop_words())

  # Generate a word cloud image
  wordcloud = WordCloud(stopwords=stopwords, background_color="white", collocations = False).generate(text)

  # Display the generated image:
  # the matplotlib way:
  plt.figure(figsize=(15,10))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis("off")
  plt.show()

  text_dictionary = wordcloud.process_text(text)
  # sort the dictionary
  word_freq={k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}

  #use words_ to print relative word frequencies
  rel_freq=wordcloud.words_

  #print results
  print(f'\n')
  print(f'Frekuensi kata paling banyak:')
  print(list(word_freq.items())[:10])
  print(f'\n')
  print(f'Frekuensi kata paling sedikit:')
  print(list(word_freq.items())[-30:-1])

  #for i, j in list(word_freq.items()):

    #if j == 1:
      #print(i)

"""### ORDER"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='ORDER'].copy().reset_index(drop=True)

word_cloud(data)

"""### SHIPPING ADDRESS"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='SHIPPING_ADDRESS'].copy().reset_index(drop=True)

word_cloud(data)

"""### CANCELLATION FEE"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='CANCELLATION_FEE'].copy().reset_index(drop=True)

word_cloud(data)

"""### INVOICE"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='INVOICE'].copy().reset_index(drop=True)

word_cloud(data)

"""### PAYMENT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='PAYMENT'].copy().reset_index(drop=True)

word_cloud(data)

"""### REFUND"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='REFUND'].copy().reset_index(drop=True)

word_cloud(data)

"""### FEEDBACK"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='FEEDBACK'].copy().reset_index(drop=True)

word_cloud(data)

"""### CONTACT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='CONTACT'].copy().reset_index(drop=True)

word_cloud(data)

"""### ACCOUNT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='ACCOUNT'].copy().reset_index(drop=True)

word_cloud(data)

"""### DELIVERY"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='DELIVERY'].copy().reset_index(drop=True)

word_cloud(data)

"""### NEWSLETTER"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='NEWSLETTER'].copy().reset_index(drop=True)

word_cloud(data)

"""## Stat test"""

from scipy.stats import chi2_contingency
from scipy.stats import chi2


chi2_check = {}


prob = 0.95
chi, p, dof, ex = chi2_contingency(pd.crosstab(train_df['category'], train_df['intent']))
chi2_check.setdefault('Feature',[]).append('category')
chi2_check.setdefault('chi',[]).append(chi)
chi2_check.setdefault('critical 0.05',[]).append(chi2.ppf(prob, dof))
chi2_check.setdefault('p-value',[]).append(round(p, 10))
chi2_check.setdefault('alpha',[]).append('0.05')

chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)
chi2_result

"""*   If Statistic >= Critical Value: significant result, reject null hypothesis (H0), dependent.
*   If Statistic < Critical Value: not significant result, fail to reject null hypothesis (H0), independent.

*   If p-value <= alpha: significant result, reject null hypothesis (H0), dependent.
*   If p-value > alpha: not significant result, fail to reject null hypothesis (H0), independent.

Berdasarkan Chi2 test, dapat diketahui bahwasanya variabel category memiliki pengaruh yang signifikan terhadap variabel target intent. Kita dapat menggunakan variabel category untuk digunakan dalam pemodelan

# Preprocessing
"""

import pandas as pd
import numpy as np
import re
import math
import string
import unicodedata

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

from sklearn.base import BaseEstimator, TransformerMixin

train_df

import json

# Specify the path to your JSON file
json_file_path = '/content/tambahan.json'

# Open the JSON file and load the data
with open(json_file_path, 'r') as json_file:
    annoying_word = json.load(json_file)

# Print the loaded data (list of strings)
print(annoying_word)

#import spacy
#
#nlp = spacy.blank("id")
#nlp.add_pipe("lemmatizer", config={"mode": "lookup"})
#nlp.initialize()

#doc = nlp('dialah')
#for token in doc:
#  print(token.text, token.lemma_)

'''idx = []

for category in df.category.unique():

  if category in list(annoying_word.keys()):
    temp = []

    for index, sentence in df['utterance'][df['category'] == category].items():

        for word in list(annoying_word[category].keys()):

            if word.lower() in sentence.lower().split():
                temp.append(index)

    idx.extend(list(set(temp)))'''

#idx.sort()

class Prepro(BaseEstimator, TransformerMixin):

    def normalisasi(self, df):
        df['utterance'] = df['utterance'].apply(lambda x: x.lower())

        for category in df.category.unique():

          if category in list(annoying_word.keys()):

            for word, replacement in annoying_word[category].items():
              df.utterance[df['category']==category] = df.utterance[df['category']==category].str.replace(r'\b{}\b'.format(word), replacement)

        return df

    def cleansing(self, text):

        text = re.sub(r"\s+", " ", text, flags=re.UNICODE)
        text = re.sub(r'[0-9]', '', text) # angka
        text = (unicodedata.normalize("NFD", text).encode("ascii", "ignore").decode("ascii"))
        text = re.sub(r'[\!\"\”\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\[\\\]\^\_\`\{\|\}\~\–]', '', text) #punctuation
        text = re.sub(r' +', ' ', text)
        text = text.strip()

        return text.lower()

    def stopwords(self, text):

        factory = StopWordRemoverFactory()
        stopword = factory.create_stop_word_remover()

        return stopword.remove(text)

    def stemming(self, text):

        factory = StemmerFactory()
        stemmer = factory.create_stemmer()

        return stemmer.stem(text)

    def fit(self, X, y=None):
        return self

    def transform(self, X, train=True):

        if train == True:
            X_copy = X.copy()  # Make a copy to avoid modifying the original DataFrame
            X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stopwords(x))
            X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stemming(x))

        else:
            X_copy = self.normalisasi(X.copy())  # Make a copy to avoid modifying the original DataFrame
            X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.cleansing(x))
            X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stopwords(x))
            X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stemming(x))

        return X_copy

valid_df = pd.read_csv('/content/drive/MyDrive/DSC dataset/df_validation.csv')

preprocessor = Prepro()
#train_df = preprocessor.transform(df, train=True)

valid_df = preprocessor.transform(valid_df, train=False)

valid_df['length'] = valid_df['utterance'].apply(lambda x: len(x.split()))

train_df

valid_df

#train_df.to_csv('/content/drive/MyDrive/DSC dataset/df_train_modeling.csv', index=False)
#train_df = pd.read_csv('/content/drive/MyDrive/DSC dataset/df_train_modeling.csv')

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_df['intent'] = le.fit_transform(train_df['intent'])
valid_df['intent'] = le.transform(valid_df['intent'])

train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle df

valid_df = valid_df.sample(frac=1, random_state=42).reset_index(drop=True)

train_df

valid_df

keys = le.classes_
values = le.transform(le.classes_)
dictionary = dict(zip(keys, values))
print(dictionary)

re = {v:k for k,v in dictionary.items()}
print(re)

from huggingface_hub import notebook_login

notebook_login()

train = train_df.rename(columns={'utterance': 'text', 'intent': 'label'})

valid = valid_df.rename(columns={'utterance': 'text', 'intent': 'label'})

train = train.drop(columns=['category', 'length'])
valid = valid.drop(columns=['category', 'tags', 'length'])

from datasets import Dataset

train_dataset = Dataset.from_dict(train)
valid_dataset = Dataset.from_dict(valid)

import datasets
dd = datasets.DatasetDict({"train":train_dataset,"validation":valid_dataset})

"""# Modeling"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length')

tokenized_datasets = dd.map(tokenize_function, batched=True)

print(tokenized_datasets['train'][0])

tokenized_datasets

tokenized_datasets = tokenized_datasets.remove_columns(
    'text'
)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

data_collator(tokenized_datasets["train"][0])

import evaluate

#accuracy = evaluate.load("accuracy")

import numpy as np

def compute_metrics(eval_pred):
    metric1 = evaluate.load("accuracy")
    metric2 = evaluate.load("precision")
    metric3 = evaluate.load("recall")

    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = metric1.compute(predictions=predictions, references=labels)['accuracy']
    precision = metric2.compute(predictions=predictions, references=labels, average='macro')["precision"]
    recall = metric3.compute(predictions=predictions, references=labels, average='macro')["recall"]
    return {"accuracy": accuracy, "precision": precision, "recall": recall}

id2label = {0: 'cancel_order', 1: 'change_order', 2: 'change_shipping_address', 3: 'check_cancellation_fee', 4: 'check_invoice', 5: 'check_payment_methods', 6: 'check_refund_policy', 7: 'complaint', 8: 'contact_customer_service', 9: 'contact_human_agent', 10: 'create_account', 11: 'delete_account', 12: 'delivery_options', 13: 'delivery_period', 14: 'edit_account', 15: 'get_invoice', 16: 'get_refund', 17: 'newsletter_subscription', 18: 'payment_issue', 19: 'place_order', 20: 'recover_password', 21: 'registration_problems', 22: 'review', 23: 'set_up_shipping_address', 24: 'switch_account', 25: 'track_order', 26: 'track_refund'}
label2id = {'cancel_order': 0, 'change_order': 1, 'change_shipping_address': 2, 'check_cancellation_fee': 3, 'check_invoice': 4, 'check_payment_methods': 5, 'check_refund_policy': 6, 'complaint': 7, 'contact_customer_service': 8, 'contact_human_agent': 9, 'create_account': 10, 'delete_account': 11, 'delivery_options': 12, 'delivery_period': 13, 'edit_account': 14, 'get_invoice': 15, 'get_refund': 16, 'newsletter_subscription': 17, 'payment_issue': 18, 'place_order': 19, 'recover_password': 20, 'registration_problems': 21, 'review': 22, 'set_up_shipping_address': 23, 'switch_account': 24, 'track_order': 25, 'track_refund': 26}

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=27, id2label=id2label, label2id=label2id
)

from transformers import TrainerCallback, TrainerControl
import numpy as np

class LossEqualityCallback(TrainerCallback):
    def __init__(self, max_loss_diff=0.01):
        self.max_loss_diff = max_loss_diff

    #def on_epoch_end(self, args, state, control, **kwargs):

        #log_history = state.log_history[-1].items()

        #return log_history


    #def on_evaluate(self, args, state, control, logs=None, **kwargs):

         #if logs is not None and 'eval_loss' in logs:
         #   log_history = self.on_epoch_end(args, state, control)  # Corrected call
         #   eval_loss = logs['eval_loss']
         #   print(log_history)
         #   print(eval_loss)

    def on_log(self, args, state, control, val={}, **kwargs):

        if 'logs' in kwargs:

          if 'loss' in kwargs['logs']:
            val.setdefault('loss', []).append(kwargs['logs']['loss'])
          elif 'eval_loss' in kwargs['logs']:
            val.setdefault('eval_loss', []).append(kwargs['logs']['eval_loss'])

        if 'loss' in val and 'eval_loss' in val:

          if len(val['loss']) == len(val['eval_loss']):
            if val['loss'][-1] < val['eval_loss'][-1] and abs(val['loss'][-1] - val['eval_loss'][-1]) > self.max_loss_diff:

              control.should_training_stop = True
              print('TRAINING STOP')

              print(f'val: {val}')
              print(state.log_history)



    '''def on_epoch_end(self, args, state, control, **kwargs):
        #validation_loss = trainer.evaluate(eval_dataset=self.trainer.eval_dataset)['eval_loss']
        log_history = state.log_history[-1].items()
        metric = state.best_metric
        print(f'log history: {log_history} metric: {metric}')
        print('='*10)
        #avg_train_loss = state.log_history[-1]['Training Loss']
        #avg_val_loss = state.log_history[-1]['Validation Loss']
#
        #if avg_train_loss < avg_val_loss:
#
        #    if abs(avg_train_loss - avg_val_loss) > self.max_loss_diff:
        #        control.should_training_stop = True
        #        print("Stopping training: Training and validation loss difference exceeded threshold.")'''

custom_callback = LossEqualityCallback()

batch_size = 16
num_train_epochs = 10
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size

training_args = TrainingArguments(
    output_dir='distilbert-base-uncased-DSC-new-cllbck',
    learning_rate=2e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    weight_decay=0.0001,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True, #ini dimatiin klo misal mau make callback
    push_to_hub=True,
    logging_steps=logging_steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[custom_callback]
)

trainer.train()

trainer.evaluate()

trainer.push_to_hub()

trainer.save_model("/content/drive/MyDrive/intent-classification-model-cllbck")

trainer.state.log_history

"""## Inference"""

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/intent-classification-model-cllbck")

tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/intent-classification-model-cllbck")

text = ["bantu nuat akun"]
encoding = tokenizer(text, return_tensors="pt")

# forward pass
outputs = model(**encoding)
predictions = outputs.logits.argmax(-1)

print(predictions)

"""# Data Augmentasi"""

df = pd.read_csv('/content/drive/MyDrive/DSC dataset/df_train.csv')

df

df = df.copy().drop_duplicates(subset=['utterance']).reset_index(drop=True)

df.duplicated().sum()

df.info()

df.isna().sum()

df = df.drop(columns=['tags'])

import json

# Specify the path to your JSON file
json_file_path = '/content/tambahan.json'

# Open the JSON file and load the data
with open(json_file_path, 'r') as json_file:
    annoying_word = json.load(json_file)

# Print the loaded data (list of strings)
print(annoying_word)

def normalisasi(df):
    df['utterance'] = df['utterance'].apply(lambda x: x.lower())

    for category in df.category.unique():

      if category in list(annoying_word.keys()):

        for word, replacement in annoying_word[category].items():
          df.utterance[df['category']==category] = df.utterance[df['category']==category].str.replace(r'\b{}\b'.format(word), replacement)

    return df

df = normalisasi(df) #apply normalisasi to df

def cleansing(text):

    text = re.sub(r"\s+", " ", text, flags=re.UNICODE)
    text = re.sub(r'[0-9]', '', text) # angka
    text = (unicodedata.normalize("NFD", text).encode("ascii", "ignore").decode("ascii"))
    text = re.sub(r'[\!\"\”\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\[\\\]\^\_\`\{\|\}\~\–]', '', text) #punctuation
    text = re.sub(r' +', ' ', text)
    text = text.strip()

    return text.lower()

df['utterance'] = df['utterance'].apply(lambda x: cleansing(x)) #apply cleansing to each row of utterance in df

df

from gensim.models import Word2Vec
import random

# Load the pre-trained Word2Vec model
model = Word2Vec.load('/content/drive/MyDrive/idwiki_word2vec.model')

def aug_word2vec(text):
  # Convert the sentence to a list of words
  words = text.split()

  # Set the number of similar words to use for augmentation
  n_similar = 3

  stopword = StopWordRemoverFactory().get_stop_words()

  # Generate augmented sentences
  augmented_sentences = []
  similar_words = {}

  for word in words:

      if word in model.wv.key_to_index and word not in stopword:

        if model.wv.most_similar(word, topn=n_similar)[-1][1] > 0.7:

            # Get the `n_similar` most similar words to the current word
            similar_word = [x[0] for x in model.wv.most_similar(word, topn=n_similar)]
            similar_words.setdefault(word, []).extend(similar_word)

  if similar_words:
    for n in range(n_similar):

      augmented_words = words.copy()

      for i, word in enumerate(words):

        if word in list(similar_words.keys()):
         augmented_words[i] = similar_words[word][n]

      augmented_sentences.append(' '.join(augmented_words))

    return augmented_sentences

def aug_text(x, intent, category):

  data = []

  for i in x:

    if aug_word2vec(i) is not None:
      data.extend(aug_word2vec(i))

  return {'utterance':data, 'intent':[intent]*len(data), 'category': category*len(data)}

# make empty pandas df and adding each colums value based on augmented text

dataset = []

for intent in list(df.intent.unique()):

  data = df.utterance[df['intent'] == intent].copy()
  category = list(df.category[df.intent == intent].unique())

  dataset.append(aug_text(data, intent, category))

columns = ['utterance', 'intent', 'category']
df_augmentasi = pd.DataFrame(columns=columns)

df_augmentasi

for data in dataset:
    df_augmentasi = df_augmentasi.append(pd.DataFrame(data), ignore_index=True)

df_augmentasi

df_augmentasi.to_csv('/content/drive/MyDrive/DSC dataset/df_augmentasi.csv', index=False)

df_augmentasi = pd.read_csv('/content/drive/MyDrive/DSC dataset/df_train_augmentasi.csv')

df_merge =  pd.concat([df, df_augmentasi], ignore_index=True, sort=False)

df_merge = df_merge.reset_index(drop=True)

df_merge

df_merge.to_csv('/content/drive/MyDrive/DSC dataset/df_train_augmentasi.csv', index=False)